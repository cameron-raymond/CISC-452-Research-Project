{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep_Model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Dl7RiS_wWELf","colab_type":"code","outputId":"131c9755-b7ce-46db-caf0-b3f9be4172ba","executionInfo":{"status":"ok","timestamp":1574787870645,"user_tz":300,"elapsed":29197,"user":{"displayName":"Gabby Bermudez","photoUrl":"","userId":"11892519826564282368"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AkacCGsc28mm","colab_type":"code","outputId":"afa2d60a-e10a-4029-a187-17a12429e5de","executionInfo":{"status":"ok","timestamp":1574787916512,"user_tz":300,"elapsed":31773,"user":{"displayName":"Gabby Bermudez","photoUrl":"","userId":"11892519826564282368"}},"colab":{"base_uri":"https://localhost:8080/","height":581}},"source":["pip install tensorflow==2.0rc0"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.0rc0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/4b/77f0965ec7e8a76d3dcd6a22ca8bbd2b934cd92c4ded43fef6bea5ff3258/tensorflow-2.0.0rc0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n","\u001b[K     |████████████████████████████████| 86.3MB 92kB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (1.12.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (1.0.8)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (0.8.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (3.10.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (1.11.2)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (0.2.2)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (1.17.4)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (1.1.0)\n","Collecting tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/28/f2a27a62943d5f041e4a6fd404b2d21cb7c59b2242a4e73b03d9ba166552/tf_estimator_nightly-1.14.0.dev2019080601-py2.py3-none-any.whl (501kB)\n","\u001b[K     |████████████████████████████████| 501kB 49.1MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (0.1.8)\n","Collecting tb-nightly<1.15.0a20190807,>=1.15.0a20190806\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/88/24b5fb7280e74c7cf65bde47c171547fd02afb3840cff41bcbe9270650f5/tb_nightly-1.15.0a20190806-py3-none-any.whl (4.3MB)\n","\u001b[K     |████████████████████████████████| 4.3MB 58.9MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (3.1.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (0.8.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (0.33.6)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0rc0) (1.15.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0rc0) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0rc0) (41.6.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0rc0) (3.1.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0rc0) (0.16.0)\n","Installing collected packages: tf-estimator-nightly, tb-nightly, tensorflow\n","  Found existing installation: tensorflow 1.15.0\n","    Uninstalling tensorflow-1.15.0:\n","      Successfully uninstalled tensorflow-1.15.0\n","Successfully installed tb-nightly-1.15.0a20190806 tensorflow-2.0.0rc0 tf-estimator-nightly-1.14.0.dev2019080601\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mUFE91DLYu3R","colab_type":"code","outputId":"9c3cb407-00a9-487e-f648-a789f387d757","executionInfo":{"status":"ok","timestamp":1574790073246,"user_tz":300,"elapsed":1985,"user":{"displayName":"Gabby Bermudez","photoUrl":"","userId":"11892519826564282368"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["import sys\n","import os\n","sys.path.append('/content/gdrive/My Drive/School/CISC_452/Project')\n","os.chdir('/content/gdrive/My Drive/School/CISC_452/Project')\n","import numpy                    as np\n","import preprocessing.preprocessing_helpers as preproc\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Dense, LSTM, Flatten, Embedding, SpatialDropout1D, GlobalMaxPool1D, Bidirectional, Dropout\n","from tensorflow.keras.models import load_model\n","import tensorflow as tf\n","import pandas as pd\n","\n","# VOCAB_SIZE = 5000\n","\n","class Toxic_Comment_LSTM(object):\n","    def __init__(self,x_train=None,y_train=None,x_test=None,y_test=None,embedding_matrix=None,max_length=None,saved_model=None):\n","        super().__init__()\n","        self.max_length     = max_length\n","        self.model          = self.define_model(embedding_matrix,saved_model=saved_model)\n","        self.x_train        = x_train\n","        self.y_train        = y_train\n","        self.x_test         = x_test\n","        self.y_test         = y_test\n","    # #  REGULAR JMODEL\n","    # def define_model(self,embedding_matrix,saved_model=None):\n","    #     if saved_model is None:\n","    #         print(\"--- Initializing Model ---\")\n","    #         VOCAB_SIZE = embedding_matrix.shape[0]\n","    #         model = Sequential()\n","    #         embedding_layer = Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n","    #         model.add(embedding_layer)\n","    #         model.add(SpatialDropout1D(0.2))\n","    #         model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","    #         model.add(Dense(6, activation='sigmoid'))\n","    #         model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    #         return model\n","    #     print(\"--- Loading Model from {} ---\".format(saved_model))\n","    #     model = preproc.load_h5_model(saved_model)\n","    #     if model is None: # If the filepath is wrong or the model hasn't actually been defined earlier\n","    #         print(\"--- no model found, initializing from scrach ---\")\n","    #         return self.define_model(embedding_matrix,saved_model=None)\n","    #     return model\n","\n","    # More complicated LSTM with additional layer\n","    def define_model(self,embedding_matrix,saved_model=None):\n","        if saved_model is None:\n","            print(\"--- Initializing Model ---\")\n","            VOCAB_SIZE = embedding_matrix.shape[0]\n","            model = Sequential()\n","            embedding_layer = Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n","            model.add(embedding_layer)\n","            model.add(SpatialDropout1D(0.2))\n","            model.add(Bidirectional(LSTM(50, dropout=0.2, recurrent_dropout=0.2)))\n","            model.add(Dense(50, activation='relu'))\n","            model.add(Dropout(0.2))\n","            model.add(Dense(6, activation='sigmoid'))\n","            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","            return model\n","        print(\"--- Loading Model from {} ---\".format(saved_model))\n","        model = preproc.load_h5_model(saved_model)\n","        if model is None: # If the filepath is wrong or the model hasn't actually been defined earlier\n","            print(\"--- no model found, initializing from scrach ---\")\n","            return self.define_model(embedding_matrix,saved_model=None)\n","        return model\n","  \n","    def train(self,x_train=None,y_train=None):\n","        x_train = self.x_train if x_train is None else x_train\n","        y_train = self.y_train if y_train is None else y_train\n","        epochs = 2\n","        batch_size = 64\n","        # callback = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001,verbose=1)\n","        history = self.model.fit(x_train,y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,verbose=1)\n","\n","    def validate(self):\n","        # Evaluate the model on the test data using `evaluate`\n","        print('\\n# Evaluate on test data')\n","        print(self.x_test.shape,self.y_test.shape)\n","        results = self.model.evaluate(self.x_test, self.y_test, batch_size=128,verbose=0)\n","        print('test loss, test acc:', results)\n","\n","        # Generate predictions (probabilities -- the output of the last layer)\n","        # on new data using `predict`\n","        shuffle_idx = np.random.choice(np.arange(self.x_test.shape[0]), 10, replace=False)\n","        x_sample, y_sample = self.x_test[shuffle_idx],self.y_test[shuffle_idx]\n","        print('\\n# Generate predictions for {} samples'.format(x_sample.shape[0]))\n","        predictions = self.model.predict(x_sample)\n","        for i,(prediction,target) in enumerate(zip(predictions,y_sample)):\n","            chosen = np.argmax(prediction)\n","            actual = np.argmax(target)\n","            print(\"\\t* prediction {} -> {}, actual -> {}\".format(i,chosen,actual))\n","            print(\"\\t\\t* {}\".format(actual))\n","        print('predictions shape:', predictions.shape)\n","\n","    def save_model(self,file_path=\"saved_models/toxic_comment_LSTM.h5\"):\n","        \n","        preproc.save_h5_model(file_path,self.model)\n","        print(\"--- model saved to {} ---\".format(file_path))\n","\n","if __name__ == \"__main__\":\n","    preds = np.loadtxt('./deep_predictions.txt')\n","    preds = preds.astype(int)\n","    print(preds)\n","    np.savetxt('./deep_predictions_int.txt', preds ,fmt=\"%d\")\n","    print('--- saved ---')\n","    # preds = preds.astype(int)\n","    # print(preds)\n","    # print(os.getcwd())\n","    # labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","\n","    # print('--- Reading test data ---')\n","    # test_df, test_max_length = preproc.return_data('./data/cleaned_test_labels.csv')\n","    # test_data = np.array(test_df['cleaned_text'])\n","    # test_labels = np.array(test_df[labels])\n","\n","    # text_df, max_length   = preproc.return_data('./data/{}.csv'.format(\"upsampled_downsampled_train\"))\n","    # train  = np.array(text_df['cleaned_text'])\n","    # t = Tokenizer(filters = '\"#$%&()*+-/:;<=>@[\\]^_`{|}~')\n","    # t.fit_on_texts(train)\n","    # train = t.texts_to_sequences(train)\n","    # train = pad_sequences(train, maxlen=max_length, padding='post')\n","\n","\n","    # # tokenize the test data\n","    # test_data = t.texts_to_sequences(test_data)\n","    # test_data = pad_sequences(test_data, maxlen=test_max_length, padding='post')\n","\n","\n","\n","    # embedding_matrix = np.load('./data/embedding_matrix.npy')\n","\n","    # mask = np.random.rand(len(train)) < 0.8\n","\n","    # x_train, x_test = train[mask], train[~mask] # split data into train test splits\n","    # y_train, y_test = np.array(text_df[mask][labels]), np.array(text_df[~mask][labels])\n","    # toxic_Comment_LSTM = Toxic_Comment_LSTM(x_test=x_test,y_test=y_test, embedding_matrix=embedding_matrix, saved_model='saved_models/toxic_comment_LSTM_deep.h5')\n","    # print('--- Predicting ---')\n","    # preds = toxic_Comment_LSTM.model.predict(test_data)\n","    preds[preds>=0.5] = int(1)\n","    preds[preds<0.5] = int(0)\n","    preds = preds.astype(int)\n","    np.savetxt('./deep_predictions.txt', preds)\n","    print('--- saved ---')\n","    print('--- Evaluating ---')\n","    toxic_Comment_LSTM.model.evaluate(test_data, test_labels)\n","\n","\n","\n","    "],"execution_count":16,"outputs":[{"output_type":"stream","text":["[[0 0 0 0 0 0]\n"," [0 0 0 0 0 0]\n"," [0 0 0 0 0 0]\n"," ...\n"," [1 0 0 0 0 0]\n"," [1 0 1 0 0 0]\n"," [0 0 0 0 0 0]]\n","--- saved ---\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_focCl99MxL7","colab_type":"text"},"source":[""]}]}